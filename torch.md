# Равномерное распределение точек на сфере #

## Часть 2. Минимизация функционала с помощью pytorch. ##

Вы читаете продолжение [публикации](). В предыдущей части было рассказано, как рассчитать потенциал системы материальных точек, которые отталкиваются друг от друга и притягиваются к поверхности единичной сферы. Здесь мы обсудим, как минимизировать эту функцию с помощью библиотеки pytorch.

![](light_torch.jpg?)

### Крибле, грабле - бумс!

Давайте посмотрим на забавные грабли, которые я учился обходить несколько дней. Повторим подробно разобранные в предыдущей статье шаги. Раскидаем случайно пять точек внутри единичного куба:

```python
n = 5
k = 1000
import torch
torch_coords_np = np.random.rand(n, 3)
```
Заведём тензор - объект библиотеки pytorch, который будет изменяться с течением времени:

```python
torch_coords_tensor = torch.from_numpy(torch_coords_np)
```

Сделаем обёртку, которая позволит алгоритму изменять тензор. Эта оболочка - аналог передачи переменной "по указателю" или "по ссылке" в C++.

```python
pt_X = torch.nn.Parameter(torch_coords_tensor)
```

И, наконец, наступаем на грабли:
```python
def create_loss_rake(pt_X):
    pt_xxt = pt_X.mm(pt_X.transpose(0, 1))
    pt_pp_sq_dist = pt_xxt.diag()
    pt_p_roll = pt_pp_sq_dist.repeat(n, 1)
    pt_q_roll = pt_pp_sq_dist.reshape(-1, 1).repeat(1, n)
    pt_pq_sq_dist = pt_p_roll + pt_q_roll - 2 * pt_xxt 
    pt_pq_dist = pt_pq_sq_dist.sqrt()
    pt_pp_dist = pt_pp_sq_dist.sqrt()
    pt_surface_dist_sq = torch.eye(n, dtype=torch.float64) + (pt_pp_dist - torch.ones(n, dtype=torch.float64)) ** 2
    pt_rec_pq_dist = 1 / pt_pq_dist - torch.eye(n, dtype=torch.float64)
    pt_ans_00 = pt_rec_pq_dist.sum()
    pt_ans_01 = k * pt_surface_dist_sq.sum()
    pt_L = (pt_ans_00 / 2 + pt_ans_01) / n
    return pt_L
```

Ну, то есть мы уже наступили. Грабли в полёте. Но по лбу ещё не дали. Чтобы неизбежное наступило, нужно попытаться вычислить  функцию потерь
```python
L = create_loss_rake(pt_X)
```

и попытаться понять, куда нам нужно идти, чтобы функция потерь как можно быстрее уменьшалась. То есть, рассчитать градиент (и потом сделать шаг в противоположную сторону):
```python
L.backward()
```
Смотрим на значение градиента:
<table><tr><td>
```python
>pt_X.gradient
tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], dtype=torch.float64)
```
</td>
<td>![](rake_face_small.jpg)</td>
</table>

Видим торжество nano-технологий. В чём же беда? Кажется, что единственное место, в котором у нас могло бы возникнуть деление на ноль, и, как результат, значение nan - это взятие обратных величин при расчёте электростатических потенциалов
```python
1 / pt_pq_dist
```
Но мы же подстраховались и в предыдущей строке добавили единичную матрицу, которая заменила все нули матрицы pt_pq_sq_dist на единицы. Оказывается, это не всё, за чем нужно следить, если мы пользуемся библиотеками pytorch и tensorflow, рассчитывающих градиент методом backpropagation. Кроме значений функций и их области определений, нужно следить и за областью определения производных. Методом деления кода пополам я выснил что производная перестаёт считаться на вызове ```sqrt```. Но ведь действительно,
$$ \left ( \sqrt x \right )' = \frac 1 {2\cdot \sqrt x} .$$
Нули на диагонали матрицы вызывают деление на ноль при вычислении производной. Переставляем простановку единиц на диагональ матрицы до вычисления корня и получаем 

```python
> pt_X.gradient
tensor([[ 29.5873,  11.8047,  18.2031],
        [ 24.6431,   4.5842,  38.6982],
        [ 54.9100,  96.8913,  77.2333],
        [101.3414, 122.9962,  93.9065],
        [-12.4928, -91.3555, -97.1234]], dtype=torch.float64)
```
Эта матрица градиента. Она указывает направление, движение в котором сильнее всего увеличит значение функции потерь. Мы будем двигаться в противоположном направлении.

